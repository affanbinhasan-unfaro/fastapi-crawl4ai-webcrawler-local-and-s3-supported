#!/usr/bin/env python3
"""
Test script to verify the updated app scraper works correctly
"""

import asyncio
import sys
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

async def test_updated_app_scraper():
    """Test the updated app scraper with HTTP-only approach"""
    print("ğŸ§ª Testing Updated App Scraper")
    print("=" * 50)
    
    try:
        # Test import
        from app.services.scraper import web_scraper
        print("âœ… Import successful!")
        
        # Test basic functionality
        print(f"ğŸ“Š Max depth: {web_scraper.max_depth}")
        print(f"â±ï¸  Timeout: {web_scraper.timeout}")
        print(f"ğŸ”§ Debug mode: {web_scraper.debug}")
        
        # Test scraping with a simple URL
        test_url = "https://httpbin.org/html"
        company_name = "test_company"
        
        print(f"\nğŸŒ Testing with URL: {test_url}")
        
        result = await web_scraper.scrape_website(
            url=test_url,
            company_name=company_name,
            max_depth=1
        )
        
        print(f"\nğŸ“Š Scraping Results:")
        print(f"  Status: SUCCESS")
        print(f"  Company: {result['metadata']['company_name']}")
        print(f"  Pages crawled: {result['metadata']['total_pages_crawled']}")
        print(f"  Processing time: {result['metadata']['processing_time_seconds']}s")
        print(f"  Method: {result['metadata']['extraction_method']}")
        
        # Check data types
        data = result['data']
        print(f"\nğŸ“ˆ Data Extracted:")
        print(f"  Text items: {len(data['text'])}")
        print(f"  Images: {len(data['images'])}")
        print(f"  Contact info: {len(data['contact'])}")
        print(f"  Products: {len(data['products'])}")
        print(f"  Social media: {len(data['social_media'])}")
        print(f"  Metadata entries: {len(data['metadata'])}")
        
        # Check raw HTML
        print(f"  Raw HTML pages: {len(result['raw_html'])}")
        
        # Coverage summary
        coverage = result['sitemap']['coverage_summary']
        print(f"\nğŸ“‹ Coverage Summary:")
        print(f"  Total pages: {coverage['total_pages']}")
        print(f"  Pages with text: {coverage['pages_with_text']}")
        print(f"  Pages with images: {coverage['pages_with_images']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error testing app scraper: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_link_extraction():
    """Test the improved link extraction"""
    print("\nğŸ”— Testing Link Extraction")
    print("=" * 30)
    
    try:
        from app.services.scraper import web_scraper
        
        # Test with a URL that has links
        test_url = "https://httpbin.org/links/3"
        company_name = "link_test"
        
        print(f"ğŸŒ Testing with: {test_url}")
        
        result = await web_scraper.scrape_website(
            url=test_url,
            company_name=company_name,
            max_depth=2
        )
        
        pages_crawled = result['metadata']['total_pages_crawled']
        print(f"ğŸ“Š Pages crawled: {pages_crawled}")
        
        if pages_crawled > 1:
            print("âœ… Link extraction and subpage crawling working!")
            
            # Show crawl structure
            sitemap = result['sitemap']['crawl_structure']
            for url, info in sitemap.items():
                print(f"  ğŸ“„ {url} (depth: {info['depth']})")
                if info['links_found']:
                    print(f"     ğŸ”— Found {len(info['links_found'])} links")
        else:
            print("âš ï¸  Only 1 page crawled - link extraction may need checking")
            
        return pages_crawled > 1
        
    except Exception as e:
        print(f"âŒ Error testing link extraction: {e}")
        return False

async def main():
    """Run all tests"""
    print("ğŸ” Updated App Scraper Test Suite")
    print("=" * 60)
    
    success_count = 0
    
    # Test 1: Basic functionality
    if await test_updated_app_scraper():
        success_count += 1
    
    # Test 2: Link extraction
    if await test_link_extraction():
        success_count += 1
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š Tests passed: {success_count}/2")
    
    if success_count == 2:
        print("ğŸ‰ All tests passed! The updated app scraper is working perfectly!")
        print("\nâœ¨ Key improvements:")
        print("  âš¡ HTTP-only crawling (much faster)")
        print("  ğŸ” BeautifulSoup link extraction (more reliable)")
        print("  ğŸ“Š Comprehensive data extraction")
        print("  ğŸŒ Recursive subpage crawling")
        print("  ğŸ”§ Same API interface (drop-in replacement)")
    else:
        print("âŒ Some tests failed - check the errors above")

if __name__ == "__main__":
    asyncio.run(main()) 